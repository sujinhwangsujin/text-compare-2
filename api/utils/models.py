# from transformers import AutoModel, AutoTokenizer

# # Check if CUDA (GPU support) is available, and set the device accordingly
# #device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
# # Load the UAE-Large-V1 model from the Hugging Face 
# model = AutoModel.from_pretrained('WhereIsAI/UAE-Large-V1')#.to(device)
# # Load the tokenizer associated with the UAE-Large-V1 model
# tokenizer = AutoTokenizer.from_pretrained('WhereIsAI/UAE-Large-V1')